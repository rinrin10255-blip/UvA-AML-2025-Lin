{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Applied Machine Learning - Week 2\n",
    "\n",
    "**Machine Learning Fundamentals: Logistic Regression, Nearest Neighbors, and Decision Trees**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "This assignment introduces fundamental machine learning algorithms. You'll implement logistic regression, nearest neighbors, and decision trees using **NumPy** only.\n",
    "\n",
    "## üìë Table of Contents\n",
    "\n",
    "1. [**Logistic Regression**](#1.-Logistic-Regression)\n",
    "   - [1.1 Linear Mapping](#1.1-Linear-Mapping)\n",
    "   - [1.2 Sigmoid](#1.2-Sigmoid)\n",
    "   - [1.3 Negative Log Likelihood](#1.3-Negative-Log-Likelihood)\n",
    "   - [1.4 Model](#1.4-Model)\n",
    "   - [1.5 Simple Experiment](#1.5-Simple-Experiment)\n",
    "2. [**Nearest Neighbors**](#2.-Nearest-Neighbors)\n",
    "   - [2.1 Distance to Training Samples](#2.1-Distance-of-input-to-training-samples)\n",
    "   - [2.2 Predicting a Label](#2.2-Predicting-a-label)\n",
    "   - [2.3 Experiment](#2.3-Experiment)\n",
    "3. [**Decision Tree**](#3.-Decision-Tree)\n",
    "   - [3.1 Entropy & Data Split](#3.1-Entropy-&-Data-Split)\n",
    "   - [3.2 Terminal Node](#3.2-Terminal-Node)\n",
    "   - [3.3 Build the Decision Tree](#3.3-Build-the-Decision-Tree)\n",
    "4. [**Experiments**](#4.-Experiments)\n",
    "   - [4.1 Decision Tree for Heart Disease Prediction](#4.1-Decision-Tree-for-Heart-Disease-Prediction)\n",
    "   - [4.2 Nearest Neighbors for Heart Disease Prediction](#4.2-Nearest-Neighbors-for-Heart-Disease-Prediction)\n",
    "   - [4.3 Logistic Regression for Heart Disease Prediction](#4.3-Logistic-Regression-for-Heart-Disease-Prediction)\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Note\n",
    "\n",
    "Some concepts below may not have been covered in lectures yet. These will be discussed in upcoming sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Before You Begin\n",
    "\n",
    "To verify your code, we use **automark**. You're registered with your student number as your username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Assignment Grade  0%              |\n",
      "| w1_L2_regression         | not attempted  |\n",
      "| w1_cal_pseudoinverse     | not attempted  |\n",
      "| w1_linear_forward        | not attempted  |\n",
      "| w2_dist_to_training_samples| not attempted  |\n",
      "| w2_linear_forward        | not attempted  |\n",
      "| w2_linear_grad_W         | not attempted  |\n",
      "| w2_linear_grad_b         | not attempted  |\n",
      "| w2_nearest_neighbors     | not attempted  |\n",
      "| w2_nll_forward           | not attempted  |\n",
      "| w2_nll_grad_input        | not attempted  |\n",
      "| w2_sigmoid_forward       | not attempted  |\n",
      "| w2_sigmoid_grad_input    | not attempted  |\n",
      "| w2_tree_split_data_left  | not attempted  |\n",
      "| w2_tree_split_data_right | not attempted  |\n",
      "| w2_tree_to_terminal      | not attempted  |\n",
      "| w2_tree_weighted_entropy | not attempted  |\n",
      "| w3_box_blur              | not attempted  |\n",
      "| w3_conv_matrix           | not attempted  |\n",
      "| w3_dense_forward         | not attempted  |\n",
      "| w3_flatten_forward       | not attempted  |\n",
      "| w3_l2_regularizer        | not attempted  |\n",
      "| w3_maxpool_forward       | not attempted  |\n",
      "| w3_relu_forward          | not attempted  |\n"
     ]
    }
   ],
   "source": [
    "import automark as am\n",
    "\n",
    "# fill in you student number as your username\n",
    "username = \"14680300\"\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Progress Tracking\n",
    "\n",
    "Initially, all tests show 'not attempted'. By the end of this notebook, you should complete all week 2 tests. Example output:\n",
    "\n",
    "```\n",
    "---------------------------------------------\n",
    "| Your name / student number                |\n",
    "| your_email@your_domain.whatever           |\n",
    "---------------------------------------------\n",
    "| Current Assignment Grade  0%              |\n",
    "---------------------------------------------\n",
    "| w2_linear_forward        | not attempted  |\n",
    "| w2_linear_grad_W         | not attempted  |\n",
    "| w2_linear_grad_b         | not attempted  |\n",
    "| w2_nll_forward           | not attempted  |\n",
    "| w2_nll_grad_input        | not attempted  |\n",
    "| w2_sigmoid_forward       | not attempted  |\n",
    "| w2_sigmoid_grad_input    | not attempted  |\n",
    "| w2_dist_to_training_samples | not attempted  |\n",
    "| w2_nearest_neighbors     | not attempted  |\n",
    "| w2_tree_weighted_entropy | not attempted  |\n",
    "| w2_tree_split_data_left  | not attempted  |\n",
    "| w2_tree_split_data_right | not attempted  |\n",
    "| w2_tree_to_terminal      | not attempted  |\n",
    "---------------------------------------------\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Important Note\n",
    "\n",
    "**Multi-week tracking:** The `am.get_progress(username)` function checks weeks 1, 2, and 3. \n",
    "\n",
    "- **Week 2 exercises** (this notebook): Linear forward, Linear dW, Linear db, Sigmoid forward, Sigmoid grad, NLL forward, NLL grad, Distance to training samples, Nearest neighbors, Tree entropy, Tree left split, Tree right split, Tree terminal\n",
    "- **Week 1 and 3** exercises appear in their respective notebooks\n",
    "- Don't worry if week 1 or 3 tests remain \"not attempted\" - focus on week 2 for now\n",
    "- All statuses should show \"completed\" when you finish all three weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (\n",
    "    print_function,\n",
    "    absolute_import,\n",
    "    division,\n",
    ")  # You don't need to know what this is.\n",
    "import numpy as np  # this imports numpy, which is used for vector- and matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° About Classes in Python\n",
    "\n",
    "This notebook uses **classes** and **instances** (already implemented). This makes code cleaner and more readable.\n",
    "\n",
    "**Learning resources:**\n",
    "- [Official Python Documentation](https://docs.python.org/3/tutorial/classes.html) \n",
    "- Video: [Object Oriented Programming Introduction](https://www.youtube.com/watch?v=ekA6hvk-8H8) by *sentdex*\n",
    "- Advanced: [Stop Writing Classes](https://www.youtube.com/watch?v=o9pEzgHorH0) - OOP antipatterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Introduction\n",
    "\n",
    "**Logistic Regression** is a generalized linear model for binary (2-class) classification.\n",
    "\n",
    "- Can be extended to multi-class and non-linear cases\n",
    "- We focus on the simplest binary case here\n",
    "\n",
    "**The Problem:**\n",
    "Given data with 2 classes (Class 0 and Class 1), logistic regression returns a probability value in [0, 1] representing the likelihood of belonging to Class 1.\n",
    "\n",
    "**Decision Boundary:**\n",
    "The set of points where prediction = 0.5 forms a line (in 2D) or hyperplane (in higher dimensions) that separates the classes.\n",
    "\n",
    "![Linear Separability](https://nlpforhackers.io/wp-content/uploads/2018/07/Linear-Separability-610x610.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ Mathematical Formulation\n",
    "\n",
    "**Model Parameters:**\n",
    "- Weight vector **W**\n",
    "- Bias **b**\n",
    "\n",
    "**Prediction for feature vector X:**\n",
    "\n",
    "$$\n",
    "f(X) = \\frac{1}{1 + \\exp(-[XW + b])} = \\sigma(h(X))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ (sigmoid function)\n",
    "- $h(X) = XW + b$ (linear transformation)\n",
    "\n",
    "**Training Objective:**\n",
    "Fit W and b by minimizing the **Negative Log-Likelihood (NLL)** on training data $\\{X_j, Y_j\\}_{j=1}^N$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{N}\\sum_j \\log\\Big[ f(X_j)^{Y_j} \\cdot (1-f(X_j))^{1-Y_j}\\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f(X_j) + (1-Y_j)\\log(1-f(X_j))\\Big]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Algorithm Steps\n",
    "\n",
    "**Computing NLL (Forward Pass):**\n",
    "\n",
    "1. **Linear mapping:** $h = XW + b$\n",
    "2. **Sigmoid activation:** $f = \\sigma(h)$\n",
    "3. **Calculate NLL:** $\\mathcal{L} = -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log f_j + (1-Y_j)\\log(1-f_j)\\Big]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìâ Gradient Descent & Backpropagation\n",
    "\n",
    "**Optimization via [Gradient Descent (GD)](https://en.wikipedia.org/wiki/Gradient_descent):**\n",
    "\n",
    "Choose a learning rate $\\gamma$ and update parameters after each NLL computation:\n",
    "\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{\\text{new}} = b_{\\text{old}} - \\gamma \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "**Computing Gradients via [Backpropagation (BP)](https://en.wikipedia.org/wiki/Backpropagation):**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial W} =\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial h} \\frac{\\partial h}{\\partial b} =\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial f} \\frac{\\partial f}{\\partial h} \\frac{\\partial h}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Linear Mapping\n",
    "\n",
    "Implement the linear transformation:\n",
    "\n",
    "$$\n",
    "h(X) = XW + b\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üìù Note on dimensionality:**\n",
    "- `n_out` = dimensionality of output (number of predictions per input)\n",
    "- For logistic regression: `n_out = 1`\n",
    "- For future assignments: `n_out > 1` (multi-output cases)\n",
    "- **Tip:** Use **NumPy** operations for generic, dimension-agnostic implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_linear_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of the linear function - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of the linear function\n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test Case\n",
    "\n",
    "Let's verify with matrices $X$, $W$, and $b$:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "W = \\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "b = \\begin{bmatrix}\n",
    "3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 1:** Compute $XW$:\n",
    "\n",
    "$$\n",
    "XW = \\begin{bmatrix}\n",
    "1 & -1 \\\\\n",
    "-1 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "2 \\\\\n",
    "-4 \\\\\n",
    "6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2:** Add bias $b$:\n",
    "\n",
    "$$\n",
    "XW + b = \n",
    "\\begin{bmatrix}\n",
    "5 \\\\\n",
    "-1 \\\\\n",
    "9 \\\\\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[1, -1], [-1, 0], [1, 1]])\n",
    "\n",
    "W_test = np.array([[4], [2]])\n",
    "\n",
    "b_test = np.array([3])\n",
    "\n",
    "h_test = w2_linear_forward(X_test, W_test, b_test)\n",
    "print(h_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_linear_forward, [\"x_input\", \"W\", \"b\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Computing Gradients\n",
    "\n",
    "Now implement the partial derivatives with respect to model parameters:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h}\n",
    "\\frac{\\partial h}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_linear_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to W parameter of the function\n",
    "        dL / dW = (dL / dh) * (dh / dW)\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer (dL / dh)\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to W parameter of the function\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_linear_grad_W, [\"x_input\", \"grad_output\", \"W\", \"b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_linear_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to b parameter of the function\n",
    "        dL / db = (dL / dh) * (dh / db)\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the linear function (dL / dh)\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to b parameter of the linear function\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_linear_grad_b, [\"x_input\", \"grad_output\", \"W\", \"b\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sigmoid\n",
    "\n",
    "The **sigmoid function** squashes values to the range [0, 1]:\n",
    "\n",
    "$$\n",
    "f = \\sigma(h) = \\frac{1}{1 + e^{-h}} \n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- Applied **element-wise** (each element independently)\n",
    "- Doesn't change tensor dimensionality\n",
    "- Shape-agnostic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_sigmoid_forward(x_input):\n",
    "    \"\"\"sigmoid nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of sigmoid layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_sigmoid_forward, [\"x_input\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîô Sigmoid Gradient (Backpropagation)\n",
    "\n",
    "Compute the gradient of the loss with respect to sigmoid input:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial h} = \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial f}\n",
    "\\frac{\\partial f}{\\partial h} \n",
    "$$\n",
    "\n",
    "**Deriving** $\\frac{\\partial f}{\\partial h}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial h} = \n",
    "\\frac{\\partial \\sigma(h)}{\\partial h} =\n",
    "\\frac{\\partial}{\\partial h} \\Big(\\frac{1}{1 + e^{-h}}\\Big)\n",
    "= \\frac{e^{-h}}{(1 + e^{-h})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{1 + e^{-h}} \\cdot \\frac{e^{-h}}{1 + e^{-h}}\n",
    "= f(h) \\cdot (1 - f(h))\n",
    "$$\n",
    "\n",
    "**Implementation steps:**\n",
    "1. Calculate $f(h) \\cdot (1 - f(h))$ \n",
    "2. Multiply element-wise by $\\frac{\\partial \\mathcal{L}}{\\partial f}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_sigmoid_grad_input(x_input, grad_output):\n",
    "    \"\"\"sigmoid nonlinearity gradient.\n",
    "        Calculate the partial derivative of the loss\n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "        grad_output: np.array of size `(n_objects, n_in)`\n",
    "            dL / df\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to the input of the function\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "        dL / dh\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_sigmoid_grad_input, [\"x_input\", \"grad_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Negative Log Likelihood (NLL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L} \n",
    "= -\\frac{1}{N}\\sum_j \\Big[ Y_j\\log \\dot{Y}_j + (1-Y_j)\\log(1-\\dot{Y}_j)\\Big]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ = number of objects\n",
    "- $Y_j$ = true label\n",
    "- $\\dot{Y}_j$ = predicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_nll_forward(target_pred, target_true):\n",
    "    \"\"\"Compute the value of NLL\n",
    "        for a given prediction and the ground truth\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects, 1)`\n",
    "        target_true: ground truth - np.array of size `(n_objects, 1)`\n",
    "    # Output\n",
    "        the value of NLL for a given prediction and the ground truth\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_nll_forward, [\"target_pred\", \"target_true\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîô NLL Gradient\n",
    "\n",
    "Calculate the partial derivative of NLL with respect to predictions:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_0} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_N}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Derivation** (step-by-step for component 0):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}_0} \n",
    "&= \\frac{\\partial}{\\partial \\dot{Y}_0} \\Big(-\\frac{1}{N}\\sum_j \\Big[ Y_j\\log \\dot{Y}_j + (1-Y_j)\\log(1-\\dot{Y}_j)\\Big]\\Big) \\\\\n",
    "&= -\\frac{1}{N} \\frac{\\partial}{\\partial \\dot{Y}_0} \\Big(Y_0\\log \\dot{Y}_0 + (1-Y_0)\\log(1-\\dot{Y}_0)\\Big) \\\\\n",
    "&= -\\frac{1}{N} \\Big(\\frac{Y_0}{\\dot{Y}_0} - \\frac{1-Y_0}{1-\\dot{Y}_0}\\Big) \\\\\n",
    "&= \\frac{1}{N} \\frac{\\dot{Y}_0 - Y_0}{\\dot{Y}_0 (1 - \\dot{Y}_0)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**General form** (element-wise operations):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\dot{Y}} = \\frac{1}{N} \\frac{\\dot{Y} - Y}{\\dot{Y} \\odot (1 - \\dot{Y})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_nll_grad_input(target_pred, target_true):\n",
    "    \"\"\"Compute the partial derivative of NLL\n",
    "        with respect to its input\n",
    "    # Arguments\n",
    "        target_pred: predictions - np.array of size `(n_objects, 1)`\n",
    "        target_true: ground truth - np.array of size `(n_objects, 1)`\n",
    "    # Output\n",
    "        the partial derivative\n",
    "        of NLL with respect to its input\n",
    "        np.array of size `(n_objects, 1)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_nll_grad_input, [\"target_pred\", \"target_true\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model\n",
    "\n",
    "Below is a complete **LogisticRegressionGD** model using the functions you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogsticRegressionGD(object):\n",
    "\n",
    "    def __init__(self, n_in, lr=0.05):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.b = np.zeros(\n",
    "            1,\n",
    "        )\n",
    "        self.W = np.random.randn(n_in, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h = w2_linear_forward(x, self.W, self.b)\n",
    "        y = w2_sigmoid_forward(self.h)\n",
    "        return y\n",
    "\n",
    "    def update_params(self, x, nll_grad):\n",
    "        # compute gradients\n",
    "        grad_h = w2_sigmoid_grad_input(self.h, nll_grad)\n",
    "        grad_W = w2_linear_grad_W(x, grad_h, self.W, self.b)\n",
    "        grad_b = w2_linear_grad_b(x, grad_h, self.W, self.b)\n",
    "        # update params\n",
    "        self.W = self.W - self.lr * grad_W\n",
    "        self.b = self.b - self.lr * grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Simple Experiment\n",
    "\n",
    "Let's test logistic regression on synthetic 2D datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "def generate_2_circles(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = 1.1 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    X2 = 3.0 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.hstack([X1, X2]).T\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def generate_2_gaussians(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = np.random.normal(loc=[1, 2], scale=[2.5, 0.9], size=(N, 2))\n",
    "    X1 = X1 @ np.array([[0.7, -0.7], [0.7, 0.7]])\n",
    "    X2 = np.random.normal(loc=[-2, 0], scale=[1, 1.5], size=(N, 2))\n",
    "    X2 = X2 @ np.array([[0.7, 0.7], [-0.7, 0.7]])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.vstack([X1, X2])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def split(X, Y, train_ratio=0.7):\n",
    "    size = len(X)\n",
    "    train_size = int(size * train_ratio)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    return X[train_indices], Y[train_indices], X[test_indices], Y[test_indices]\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "\n",
    "X, Y = generate_2_circles()\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"none\")\n",
    "ax1.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "X, Y = generate_2_gaussians()\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"none\")\n",
    "ax2.set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train our model\n",
    "model = LogsticRegressionGD(2, 0.05)\n",
    "\n",
    "for step in range(100):\n",
    "    Y_pred = model.forward(X_train)\n",
    "\n",
    "    loss_value = w2_nll_forward(Y_pred, Y_train)\n",
    "    accuracy = ((Y_pred > 0.5) == Y_train).mean()\n",
    "    print(\n",
    "        \"Step: {} \\t Loss: {:.3f} \\t Acc: {:.1f}%\".format(\n",
    "            step, loss_value, accuracy * 100\n",
    "        )\n",
    "    )\n",
    "\n",
    "    loss_grad = w2_nll_grad_input(Y_pred, Y_train)\n",
    "    model.update_params(X_train, loss_grad)\n",
    "\n",
    "\n",
    "print(\"\\n\\nTesting...\")\n",
    "Y_test_pred = model.forward(X_test)\n",
    "test_accuracy = ((Y_test_pred > 0.5) == Y_test).mean()\n",
    "print(\"Acc: {:.1f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_prediction(prediction_func, X, Y, hard=True):\n",
    "    u_min = X[:, 0].min() - 1\n",
    "    u_max = X[:, 0].max() + 1\n",
    "    v_min = X[:, 1].min() - 1\n",
    "    v_max = X[:, 1].max() + 1\n",
    "\n",
    "    U, V = np.meshgrid(np.linspace(u_min, u_max, 100), np.linspace(v_min, v_max, 100))\n",
    "    UV = np.stack([U.ravel(), V.ravel()]).T\n",
    "    c = prediction_func(UV).ravel()\n",
    "    if hard:\n",
    "        c = c > 0.5\n",
    "    plt.scatter(UV[:, 0], UV[:, 1], c=c, edgecolors=\"none\", alpha=0.15)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"black\")\n",
    "    plt.xlim(left=u_min, right=u_max)\n",
    "    plt.ylim(bottom=v_min, top=v_max)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_model_prediction(lambda x: model.forward(x), X_train, Y_train, False)\n",
    "\n",
    "plot_model_prediction(lambda x: model.forward(x), X_train, Y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the same experiment on 2 circles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nearest Neighbors\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Introduction\n",
    "\n",
    "**k-Nearest Neighbors (k-NN)** is a **non-parametric** algorithm:\n",
    "- Unlike Logistic Regression, there are **no trainable parameters**\n",
    "- Classification based on similarity to training examples\n",
    "- Predicts labels by \"voting\" among k nearest neighbors\n",
    "\n",
    "**Algorithm:**\n",
    "1. Find the k training samples most similar to the input\n",
    "2. Predict the most frequent label among these k neighbors\n",
    "\n",
    "**For this assignment:** We implement the simple case where **k = 1** (single nearest neighbor)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Distance to Training Samples\n",
    "\n",
    "To find the nearest neighbor, we first compute distances from the input to all training samples.\n",
    "\n",
    "**Distance metric:** [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sqrt{\\sum_{i} (x_i - y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_dist_to_training_samples(x_input, training_set):\n",
    "    \"\"\"Calculate distance between an input sample and the N training samples.\n",
    "    # Arguments\n",
    "        x_input: samples for which we want to make a predicton\n",
    "            np.array of size `(n_in,)`\n",
    "        training_set: all our training samples\n",
    "            np.array of size `(N, n_in)`\n",
    "    # Output\n",
    "        The distances between our input samples and training samples\n",
    "        np.array of size `(N,)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_dist_to_training_samples, [\"x_input\", \"training_set\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Predicting a Label\n",
    "\n",
    "**Steps:**\n",
    "1. Find the nearest neighbor (training sample with minimum distance)\n",
    "2. Predict by taking the label of this nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_nearest_neighbors(distances, training_labels):\n",
    "    \"\"\"Predict the label of the input sample given the distances of\n",
    "        this sample to the training samples and the labels of the\n",
    "        training samples.\n",
    "    # Arguments\n",
    "        distances: distances from the input sample to the N training samples\n",
    "            np.array of size `(N,)`\n",
    "        training_labels: true labels of the training samples\n",
    "            np.array of size `(N,)`\n",
    "    # Output\n",
    "        prediction:\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_nearest_neighbors, [\"distances\", \"training_labels\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Experiment\n",
    "\n",
    "Let's test the nearest neighbor method on our toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors_pred_label(x_input, training_set, training_labels):\n",
    "    distances = w2_dist_to_training_samples(x_input, training_set)\n",
    "    return w2_nearest_neighbors(distances, training_labels)\n",
    "\n",
    "\n",
    "Y_pred = np.apply_along_axis(\n",
    "    lambda x: nearest_neighbors_pred_label(x, X_train, Y_train), axis=1, arr=X_test\n",
    ")\n",
    "accuracy = ((Y_pred > 0.5) == Y_test).mean()\n",
    "\n",
    "print(f\"{100 * accuracy:.1f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Discussion Question\n",
    "\n",
    "**Why is there a large performance difference between logistic regression and nearest neighbors on the 2 circles dataset?**\n",
    "\n",
    "Think about:\n",
    "- What kind of decision boundaries can logistic regression learn?\n",
    "- What kind of decision boundaries can nearest neighbors create?\n",
    "- Which is better suited for circular patterns?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree\n",
    "\n",
    "---\n",
    "\n",
    "### üå≥ Introduction\n",
    "\n",
    "**Decision Trees** are **non-parametric** models (like k-NN):\n",
    "- No trainable parameters to learn\n",
    "- Create hierarchical decision rules\n",
    "- Easy to interpret and visualize\n",
    "\n",
    "**Example:** Credit Decision Tree\n",
    "\n",
    "![Credit Decision Tree](images/creditdecisiontree.png)\n",
    "\n",
    "**How it works:**\n",
    "- Each node (except leaves) asks a question about features\n",
    "- Navigate from root to leaf based on feature values\n",
    "- Leaf nodes provide final predictions\n",
    "\n",
    "**Features in the credit example:**\n",
    "1. Checking account balance\n",
    "2. Duration of requested credit\n",
    "3. Payment status of previous loan\n",
    "4. Length of current employment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî® Building a Decision Tree\n",
    "\n",
    "**Construction algorithm** (starting from root node):\n",
    "\n",
    "1. **Choose splitting criterion** - Select best feature and threshold\n",
    "2. **Split dataset** - Divide into two groups based on criterion\n",
    "3. **Add child nodes** - One for each split\n",
    "4. **For each child, decide:**\n",
    "   - **A:** Repeat from step 1 (create another split)\n",
    "   - **B:** Make it a leaf node (predict by majority vote)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Entropy & Data Split\n",
    "\n",
    "**Choosing good splitting rules:**\n",
    "\n",
    "Two key criteria:\n",
    "1. **Informativeness** - Does the rule help make decisions?\n",
    "2. **Generalizability** - Will it work on unseen data?\n",
    "\n",
    "Following [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor): simpler is better.\n",
    "\n",
    "**Quality measure:** Weighted Entropy\n",
    "\n",
    "$$\n",
    "E(S) = \\sum_{i\\in \\{L, R\\}} \\frac{|S_i|}{|S|} E(S_i)\n",
    "$$\n",
    "\n",
    "where $S_L$ and $S_R$ are left and right splits, and:\n",
    "\n",
    "$$\n",
    "E(S_i) = -\\sum_{j=1}^n P_j \\log_2 (P_j)\n",
    "$$\n",
    "\n",
    "- $P_j$ = fraction of class j in the split\n",
    "- $n$ = number of classes (for binary: n = 2)\n",
    "- **Lower entropy = better split**\n",
    "- **Perfect split:** Entropy = 0 (all labels identical in each split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_tree_weighted_entropy(Y_left, Y_right, classes):\n",
    "    \"\"\"Compute the weighted entropy.\n",
    "    # Arguments\n",
    "        Y_left: class labels of the data left set\n",
    "            np.array of size `(n_objects, 1)`\n",
    "        Y_right: class labels of the data right set\n",
    "            np.array of size `(n_objects, 1)`\n",
    "        classes: list of all class values\n",
    "    # Output\n",
    "        weighted_entropy: scalar `float`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_tree_weighted_entropy, [\"Y_left\", \"Y_right\", \"classes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÇ Splitting Data\n",
    "\n",
    "At each node, data is partitioned by a split criterion:\n",
    "- **Left child:** Examples where feature < split_value\n",
    "- **Right child:** Examples where feature ‚â• split_value\n",
    "\n",
    "Implement functions to return the appropriate subset for each child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_tree_split_data_left(X, Y, feature_index, split_value):\n",
    "    \"\"\"Split the data `X` and `Y`, at the feature indexed by `feature_index`.\n",
    "    If the value is less than `split_value` then return it as part of the left group.\n",
    "\n",
    "    # Arguments\n",
    "        X: np.array of size `(n_objects, n_in)`\n",
    "        Y: np.array of size `(n_objects, 1)`\n",
    "        feature_index: index of the feature to split at\n",
    "        split_value: value to split between\n",
    "    # Output\n",
    "        (XY_left): np.array of size `(n_objects_left, n_in + 1)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return XY_left\n",
    "\n",
    "\n",
    "def w2_tree_split_data_right(X, Y, feature_index, split_value):\n",
    "    \"\"\"Split the data `X` and `Y`, at the feature indexed by `feature_index`.\n",
    "    If the value is greater or equal than `split_value` then return it as part of the right group.\n",
    "\n",
    "    # Arguments\n",
    "        X: np.array of size `(n_objects, n_in)`\n",
    "        Y: np.array of size `(n_objects, 1)`\n",
    "        feature_index: index of the feature to split at\n",
    "        split_value: value to split between\n",
    "    # Output\n",
    "        (XY_left): np.array of size `(n_objects_left, n_in + 1)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return XY_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_tree_split_data_left, [\"X\", \"Y\", \"feature_index\", \"split_value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(\n",
    "    username, w2_tree_split_data_right, [\"X\", \"Y\", \"feature_index\", \"split_value\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Finding the Best Split\n",
    "\n",
    "We search for the split rule with lowest weighted entropy by:\n",
    "- Trying all features\n",
    "- Trying all possible split values\n",
    "- Computing entropy for each combination\n",
    "- Selecting the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_best_split(X, Y):\n",
    "    class_values = list(set(Y.flatten().tolist()))\n",
    "    r_index, r_value, r_score = float(\"inf\"), float(\"inf\"), float(\"inf\")\n",
    "    r_XY_left, r_XY_right = (X, Y), (X, Y)\n",
    "    for feature_index in range(X.shape[1]):\n",
    "        for row in X:\n",
    "            XY_left = w2_tree_split_data_left(X, Y, feature_index, row[feature_index])\n",
    "            XY_right = w2_tree_split_data_right(X, Y, feature_index, row[feature_index])\n",
    "            XY_left, XY_right = (XY_left[:, :-1], XY_left[:, -1:]), (\n",
    "                XY_right[:, :-1],\n",
    "                XY_right[:, -1:],\n",
    "            )\n",
    "            entropy = w2_tree_weighted_entropy(XY_left[1], XY_right[1], class_values)\n",
    "            if entropy < r_score:\n",
    "                r_index, r_value, r_score = feature_index, row[feature_index], entropy\n",
    "                r_XY_left, r_XY_right = XY_left, XY_right\n",
    "    return {\n",
    "        \"index\": r_index,\n",
    "        \"value\": r_value,\n",
    "        \"XY_left\": r_XY_left,\n",
    "        \"XY_right\": r_XY_right,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Terminal Node (Leaf)\n",
    "\n",
    "**Leaf nodes predict labels** by majority voting:\n",
    "- Take all training labels that reached this leaf\n",
    "- Return the most frequent label as the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2_tree_to_terminal(Y):\n",
    "    \"\"\"The most frequent class label, out of the data points belonging to the leaf node,\n",
    "    is selected as the predicted class.\n",
    "\n",
    "    # Arguments\n",
    "        Y: np.array of size `(n_objects,1)`\n",
    "\n",
    "    # Output\n",
    "        label: most frequent label of `Y.dtype`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w2_tree_to_terminal, [\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build the Decision Tree\n",
    "\n",
    "Recursively build the tree by greedily splitting at each node based on entropy.\n",
    "\n",
    "**Preventing overfitting** - Convert to terminal node if:\n",
    "1. **Maximum depth** is reached\n",
    "2. Node has fewer than **minimum samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_recursive_split(X, Y, node, max_depth, min_size, depth):\n",
    "    XY_left, XY_right = node[\"XY_left\"], node[\"XY_right\"]\n",
    "    del node[\"XY_left\"]\n",
    "    del node[\"XY_right\"]\n",
    "    # check for a no split\n",
    "    if XY_left[0].size <= 0 or XY_right[0].size <= 0:\n",
    "        node[\"left_child\"] = node[\"right_child\"] = w2_tree_to_terminal(\n",
    "            np.concatenate((XY_left[1], XY_right[1]))\n",
    "        )\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node[\"left_child\"], node[\"right_child\"] = w2_tree_to_terminal(\n",
    "            XY_left[1]\n",
    "        ), w2_tree_to_terminal(XY_right[1])\n",
    "        return\n",
    "    # process left child\n",
    "    if XY_left[0].shape[0] <= min_size:\n",
    "        node[\"left_child\"] = w2_tree_to_terminal(XY_left[1])\n",
    "    else:\n",
    "        node[\"left_child\"] = tree_best_split(*XY_left)\n",
    "        tree_recursive_split(X, Y, node[\"left_child\"], max_depth, min_size, depth + 1)\n",
    "    # process right child\n",
    "    if XY_right[0].shape[0] <= min_size:\n",
    "        node[\"right_child\"] = w2_tree_to_terminal(XY_right[1])\n",
    "    else:\n",
    "        node[\"right_child\"] = tree_best_split(*XY_right)\n",
    "        tree_recursive_split(X, Y, node[\"right_child\"], max_depth, min_size, depth + 1)\n",
    "\n",
    "\n",
    "def build_tree(X, Y, max_depth, min_size):\n",
    "    root = tree_best_split(X, Y)\n",
    "    tree_recursive_split(X, Y, root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñ®Ô∏è Visualizing the Tree\n",
    "\n",
    "Print split criteria or predicted classes at each node to understand decision-making.\n",
    "\n",
    "**Prediction:** Recursively traverse from root to leaf node based on feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print(\"%s[X%d < %.3f]\" % ((depth * \" \", (node[\"index\"] + 1), node[\"value\"])))\n",
    "        print_tree(node[\"left_child\"], depth + 1)\n",
    "        print_tree(node[\"right_child\"], depth + 1)\n",
    "    else:\n",
    "        print(\"%s[%s]\" % ((depth * \" \", node)))\n",
    "\n",
    "\n",
    "def tree_predict_single(x, node):\n",
    "    if isinstance(node, dict):\n",
    "        if x[node[\"index\"]] < node[\"value\"]:\n",
    "            return tree_predict_single(x, node[\"left_child\"])\n",
    "        else:\n",
    "            return tree_predict_single(x, node[\"right_child\"])\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def tree_predict_multi(X, node):\n",
    "    Y = np.array([tree_predict_single(row, node) for row in X])\n",
    "    return Y[:, None]  # size: (n_object,) -> (n_object, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Testing the Decision Tree\n",
    "\n",
    "Let's test on toy data. **Note:** Ensure your `w2_tree_weighted_entropy` function handles empty splits correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)\n",
    "\n",
    "tree = build_tree(X_train, Y_train, 4, 1)\n",
    "Y_pred = tree_predict_multi(X_test, tree)\n",
    "test_accuracy = (Y_pred == Y_test).mean()\n",
    "print(\"Test Acc: {:.1f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree structure** printed in [pre-order traversal](https://en.wikipedia.org/wiki/Tree_traversal#Pre-order_(NLR)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_prediction(lambda x: tree_predict_multi(x, tree), X_test, Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments\n",
    "\n",
    "---\n",
    "\n",
    "### üè• Cleveland Heart Disease Dataset\n",
    "\n",
    "The [Cleveland Heart Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) dataset predicts heart disease presence based on medical information.\n",
    "\n",
    "**Dataset details:**\n",
    "- Full database: 76 attributes\n",
    "- **We use: 14 key attributes**\n",
    "\n",
    "**Features (1-13):**\n",
    "\n",
    "1. **Age** - Age in years \n",
    "2. **Sex** - 0 = female, 1 = male \n",
    "3. **Chest pain type:**\n",
    "   - 1 = typical angina\n",
    "   - 2 = atypical angina\n",
    "   - 3 = non-anginal pain\n",
    "   - 4 = asymptomatic\n",
    "4. **Trestbps** - Resting blood pressure (mm Hg)\n",
    "5. **Chol** - Serum cholesterol (mg/dl) \n",
    "6. **Fasting blood sugar** - > 120 mg/dl (0 = false, 1 = true)\n",
    "7. **Resting ECG results:**\n",
    "   - 0 = normal\n",
    "   - 1 = ST-T wave abnormality\n",
    "   - 2 = left ventricular hypertrophy\n",
    "8. **Thalach** - Maximum heart rate achieved \n",
    "9. **Exercise induced angina** - 0 = no, 1 = yes\n",
    "10. **Oldpeak** - ST depression (exercise vs. rest) \n",
    "11. **Slope** - Peak exercise ST segment slope:\n",
    "    - 1 = upsloping, 2 = flat, 3 = downsloping \n",
    "12. **Ca** - Number of major vessels (0-3) colored by fluoroscopy \n",
    "13. **Thal:**\n",
    "    - 3 = normal, 6 = fixed defect, 7 = reversible defect \n",
    "\n",
    "**Target (14):**\n",
    "14. **Heart disease diagnosis** (angiographic disease status):\n",
    "    - 0 = < 50% diameter narrowing (no disease)\n",
    "    - 1 = > 50% diameter narrowing (disease present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Data Preparation\n",
    "\n",
    "Helper functions for downloading and preprocessing are in `heart_disease_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heart_disease_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = heart_disease_data.download_and_preprocess()\n",
    "X_train, Y_train, X_test, Y_test = split(X, Y, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üëÄ Exploring the Data\n",
    "\n",
    "Let's examine some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0:2])\n",
    "print(Y_train[0:2])\n",
    "\n",
    "# TODO feel free to explore more examples and see if you can predict the presence of a heart disease"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Decision Tree for Heart Disease Prediction\n",
    "\n",
    "Let's build a decision tree and evaluate performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you are free to make use of code that we provide in previous cells\n",
    "# TODO: play around with different hyper parameters and see how these impact your performance\n",
    "\n",
    "tree = build_tree(X_train, Y_train, 5, 4)\n",
    "Y_pred = tree_predict_multi(X_test, tree)\n",
    "test_accuracy = (Y_pred == Y_test).mean()\n",
    "print(\"Test Acc: {:.1f}%\".format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Hyperparameter Tuning\n",
    "\n",
    "**Question:** How did changing hyperparameters affect test performance?\n",
    "\n",
    "**Note:** Hyperparameters should be tuned using a [validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Validation_dataset), not the test set, to avoid overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Nearest Neighbors for Heart Disease Prediction\n",
    "\n",
    "Test k-NN with k=1 on the heart disease data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.apply_along_axis(\n",
    "    lambda x: nearest_neighbors_pred_label(x, X_train, Y_train), axis=1, arr=X_test\n",
    ")\n",
    "accuracy = ((Y_pred > 0.5) == Y_test).mean()\n",
    "\n",
    "print(f\"{100 * accuracy:.1f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Logistic Regression for Heart Disease Prediction\n",
    "\n",
    "Train a logistic regression model to find correlations automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you are free to make use of code that we provide in previous cells\n",
    "# TODO: play around with different hyper parameters and see how these impact your performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Performance Analysis\n",
    "\n",
    "**Question:** Was your model better than random guessing?\n",
    "\n",
    "Let's check the empirical mean of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Investigating the Problem\n",
    "\n",
    "Let's examine the learned parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.W, model.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéöÔ∏è Feature Scaling\n",
    "\n",
    "**Observation:** Some weights are much larger than others after training.\n",
    "\n",
    "**Analysis:**\n",
    "- Compare weight initialization range\n",
    "- Compare learning rate (step size)\n",
    "- Compare input feature scales\n",
    "\n",
    "**The problem:** Features have different scales!\n",
    "\n",
    "**Solution:** [Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "\n",
    "Rescale input features so they have similar ranges. This helps gradient descent converge faster and more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rescale the input features and train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Final Discussion Question\n",
    "\n",
    "**Why didn't decision trees need feature scaling?**\n",
    "\n",
    "Think about how decision trees make splits versus how gradient descent optimizes parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
